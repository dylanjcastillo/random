{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a008f07f",
   "metadata": {},
   "source": [
    "# üìö Text classification using Python and Scikit-learn\n",
    "\n",
    "Text classification is a technique that automatically assigns labels to pieces of text, such as articles, blog posts, or reviews. Many businesses use this because it allows you to organize and analyze text data without the need for manual labor. This blog post will teach you how to classify text using Python and the Scikit-learn library.\n",
    "\n",
    "But first, you might be wondering why learning how to classify text with python and scikit-learn is important. After all, there are many ways to classify text, so what's the big deal?\n",
    "\n",
    "Well, the thing is, text classification is a very powerful tool. This technique is responsible for keeping your email free of spam, assisting authors in detecting plagiarism, and helping your grammar corrector in understanding the various parts of speech.\n",
    "\n",
    "And the simplest way to do it is with Python and Scikit-learn! You can be up and running in no time with a little effort.\n",
    "\n",
    "In the next sections, I'll show you how. Let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ae87f",
   "metadata": {},
   "source": [
    "## 0Ô∏è‚É£ Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10218fda",
   "metadata": {},
   "source": [
    "1. Create a virtual environment using `conda` or `venv`\n",
    "2. Install the required libraries: `pip install numpy pandas notebook scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004d23f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ccb47cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c7d92",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d1219",
   "metadata": {},
   "source": [
    "First, let's start by reading the data. We'll use a data sample included in `scikit-learn` called **20 news groups**.\n",
    "\n",
    "Use this code to read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "27af9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"misc.forsale\",\n",
    "    \"sci.space\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"talk.politics.guns\",\n",
    "]\n",
    "\n",
    "news_group_data = fetch_20newsgroups(\n",
    "    subset=\"all\", remove=(\"headers\", \"footers\", \"quotes\"), categories=categories\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(dict(text=news_group_data[\"data\"], target=news_group_data[\"target\"]))\n",
    "df[\"target\"] = df.target.map(lambda x: categories[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a9b049",
   "metadata": {},
   "source": [
    "This code reads the 20 news groups dataset. Here's how it works:\n",
    "- **Lines 1 to 7:** Define a list of categories, which are the different types of newsgroups that will be used in the analysis.\n",
    "- **Lines 9 to 11:** Use the `fetch_20newsgroups` function to get data from the 20 news groups dataset. This function removes the headers, footers, and quotes from the data, and only gets data from the categories that are specified in the `categories` list.\n",
    "- **Lines 13 and 14:** Create a dataframe from the data that was fetched. The dataframe has two columns, one for the text of the newsgroup post and one for the category (target) of the newsgroup. You change the target column so that it displays the actual category name instead of a number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7f0b53",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Clean the text column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a750af",
   "metadata": {},
   "source": [
    "Next, you'll clean the text to remove the punctuation marks and multiple spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "709c4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \" \", text\n",
    "    )\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df.text.map(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35525ee",
   "metadata": {},
   "source": [
    "This code lowercases the text and removes any punctuation marks or duplicated spaces and stores the results in a new column called `clean_text`. For that, you use the function `process_text`, which takes a string as input, lowercases it, replaces all punctuation marks with spaces, and removes the duplicated spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac64f0",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f07d8d",
   "metadata": {},
   "source": [
    "Next, you'll split the dataset into a training and a testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b51609dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.20, stratify=df.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edc02ab",
   "metadata": {},
   "source": [
    "The `train_test_split` function is used to split a dataset into a training set and a testing set. You provide the dataframe you wish to split and specify the following parameters:\n",
    "- `test_size`: size of the testing set (as a decimal fraction of the total dataset).\n",
    "- `stratify`: ensures that the training and testing sets are split in a stratified manner, meaning that the proportion of each class in the dataset is preserved in both sets.\n",
    "\n",
    "Next, you'll use these datasets to train and evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4962b4ad",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Create bag-of-words features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82989d07",
   "metadata": {},
   "source": [
    "Machine Learning models cannot handle text features directly. To train your models you'll need to turn your text into numerical features. You'll use `CountVectorizer` for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "107b3267",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(\n",
    "    ngram_range=(1, 3), \n",
    "    stop_words=\"english\",\n",
    ")\n",
    "\n",
    "X_train = vec.fit_transform(df_train.clean_text)\n",
    "X_test = vec.transform(df_test.clean_text)\n",
    "\n",
    "y_train = df_train.target\n",
    "y_test = df_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893e66f",
   "metadata": {},
   "source": [
    "In the code above you used `CountVectorizer` to turn the text into numerical features. Here's what's happening:\n",
    "\n",
    "- **Lines 1 to 4:** You use `CountVectorizer` to build a [bag-of-words representation](https://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation) of the `clean_text` column so that a machine learning model can understand it. You specify two paramters: `ngram_range` and `stop_words`. `ngram_range` is the range of n-grams that the function will use. An n-gram is a sequence of n words. `(1, 3)` means that the function will use sequences of 1, 2, and 3 words. `stop_words` is a list of words that the function will ignore. In this case, the list \"english\" means that the function will ignore most common words in English.\n",
    "- **Lines 6 and 7:** You generate the matrices of token counts (bag-of-words) for your training and testing set and save them into `X_train` and `X_test`.\n",
    "- **Lines 9 and 10:** You save the response variable from the training and testing set into `y_train` and `y_test`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95546155",
   "metadata": {},
   "source": [
    "### Train and evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f2d4df",
   "metadata": {},
   "source": [
    "Finally, you just need to train the model by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b39f29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.97      0.53      0.68       160\n",
      "          misc.forsale       0.98      0.89      0.94       195\n",
      "             sci.space       0.91      0.88      0.89       197\n",
      "soc.religion.christian       0.65      0.99      0.79       200\n",
      "    talk.politics.guns       0.92      0.88      0.90       182\n",
      "\n",
      "              accuracy                           0.85       934\n",
      "             macro avg       0.89      0.83      0.84       934\n",
      "          weighted avg       0.88      0.85      0.84       934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "preds = nb.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308c834",
   "metadata": {},
   "source": [
    "In **lines 1 and 2** you train a [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) model. This is a simple probabilistic model that is commonly used when using discrete features such as word counts.\n",
    "\n",
    "Then, in **lines 4 and 5**, you evaluate the results of the model by computing the precision, recall, and f1 scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa18ad",
   "metadata": {},
   "source": [
    "### Saving and loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbed3cf",
   "metadata": {},
   "source": [
    "If you'd like to save the model for later, then use you can use `joblib`. Here's how you'd save the model you just finished training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "41d19ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nb.joblib']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(nb, \"nb.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2221a1",
   "metadata": {},
   "source": [
    "Then, if you want to re-use your model later on, you can simply read it and use it to classify new samples of data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3375c2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sci.space'], dtype='<U22')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_loaded = joblib.load(\"nb.joblib\")\n",
    "\n",
    "sample_text = [\"space stars planets astronomy\"]\n",
    "sample_vec = vec.transform(sample_text)\n",
    "nb_loaded.predict(sample_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c660d",
   "metadata": {},
   "source": [
    "## Using Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8b397351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1=0.84+/-0.04 accuracy=0.84 kappa=0.80\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "kappa_scores = []\n",
    "\n",
    "for f, (t, v) in enumerate(kf.split(X=df, y=df.target)):\n",
    "    df_train = df.iloc[t, :]\n",
    "    df_val = df.iloc[v, :]\n",
    "    \n",
    "    vec = CountVectorizer(\n",
    "        ngram_range=(1, 3), \n",
    "        stop_words=\"english\"\n",
    "    )\n",
    "    \n",
    "    X_train = vec.fit_transform(df_train.clean_text)\n",
    "    y_train = df_train.target\n",
    "    \n",
    "    X_val = vec.transform(df_val.clean_text)\n",
    "    y_val = df_val.target\n",
    "\n",
    "    nb = MultinomialNB()\n",
    "\n",
    "    nb.fit(X_train, y_train)\n",
    "    preds = nb.predict(X_val)\n",
    "    \n",
    "    f1_scores.append(f1_score(y_val, preds, average=\"macro\"))\n",
    "    acc_scores.append(accuracy_score(y_val, preds))\n",
    "    kappa_scores.append(cohen_kappa_score(y_val, preds))\n",
    "    \n",
    "print(\n",
    "    f\"f1={np.mean(f1_scores):.2f}\",\n",
    "    f\"accuracy={np.mean(acc_scores):.2f}\",\n",
    "    f\"kappa={np.mean(kappa_scores):.2f}\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
