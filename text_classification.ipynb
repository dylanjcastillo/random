{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a008f07f",
   "metadata": {},
   "source": [
    "# üìö Text classification using Python and Scikit-learn\n",
    "\n",
    "Text classification is the task of automatically assigning labels to pieces of text, such as articles, blog posts, or reviews. Many businesses use text classification algorithms to save time and money by reducing the amount of manual labor needed to organize and analyze their text data.\n",
    "\n",
    "These algorithms are extremely powerful tools when used correctly. Text classification models keep your email free of spam, assist authors in detecting plagiarism, and help your grammar checker understand the various parts of speech.\n",
    "\n",
    "If you want to build a text classifier, you have many options to choose from. You can use traditional methods such as bag-of-words, advanced methods like Word2Vec embeddings, and cutting-edge approaches like BERT or GPT-3.\n",
    "\n",
    "However, if you want to get something up and running quickly at no cost, you should build your text classification model with Python and Scikit-learn. I'll show you how in this tutorial.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ae87f",
   "metadata": {},
   "source": [
    "## 0Ô∏è‚É£ Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10218fda",
   "metadata": {},
   "source": [
    "1. Create a virtual environment using `conda` or `venv`\n",
    "2. Install the required libraries: `pip install numpy pandas notebook scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004d23f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb47cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c7d92",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d1219",
   "metadata": {},
   "source": [
    "First, let's start by reading the data. We'll use a data sample included in `scikit-learn` called **20 news groups**.\n",
    "\n",
    "Use this code to read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27af9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"misc.forsale\",\n",
    "    \"sci.space\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"talk.politics.guns\",\n",
    "]\n",
    "\n",
    "news_group_data = fetch_20newsgroups(\n",
    "    subset=\"all\", remove=(\"headers\", \"footers\", \"quotes\"), categories=categories\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(dict(text=news_group_data[\"data\"], target=news_group_data[\"target\"]))\n",
    "df[\"target\"] = df.target.map(lambda x: categories[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a9b049",
   "metadata": {},
   "source": [
    "This code reads the 20 news groups dataset. Here's how it works:\n",
    "- **Lines 1 to 7:** Define a list of categories, which are the different types of newsgroups that will be used in the analysis.\n",
    "- **Lines 9 to 11:** Use the `fetch_20newsgroups` function to get data from the 20 news groups dataset. This function removes the headers, footers, and quotes from the data, and only gets data from the categories that are specified in the `categories` list.\n",
    "- **Lines 13 and 14:** Create a dataframe from the data that was fetched. The dataframe has two columns, one for the text of the newsgroup post and one for the category (target) of the newsgroup. You change the target column so that it displays the actual category name instead of a number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7f0b53",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Clean the text column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a750af",
   "metadata": {},
   "source": [
    "Next, you'll clean the text to remove the punctuation marks and multiple spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709c4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \" \", text\n",
    "    )\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df.text.map(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35525ee",
   "metadata": {},
   "source": [
    "This code lowercases the text and removes any punctuation marks or duplicated spaces and stores the results in a new column called `clean_text`. For that, you use the function `process_text`, which takes a string as input, lowercases it, replaces all punctuation marks with spaces, and removes the duplicated spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac64f0",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f07d8d",
   "metadata": {},
   "source": [
    "Next, you'll split the dataset into a training and a testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51609dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.20, stratify=df.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edc02ab",
   "metadata": {},
   "source": [
    "The `train_test_split` function is used to split a dataset into a training set and a testing set. You provide the dataframe you wish to split and specify the following parameters:\n",
    "- `test_size`: size of the testing set (as a decimal fraction of the total dataset).\n",
    "- `stratify`: ensures that the training and testing sets are split in a stratified manner, meaning that the proportion of each class in the dataset is preserved in both sets.\n",
    "\n",
    "Next, you'll use these datasets to train and evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4962b4ad",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Create bag-of-words features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82989d07",
   "metadata": {},
   "source": [
    "Machine Learning models cannot handle text features directly. To train your models you'll need to turn your text into numerical features. You'll use `CountVectorizer` for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "107b3267",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(\n",
    "    ngram_range=(1, 3), \n",
    "    stop_words=\"english\",\n",
    ")\n",
    "\n",
    "X_train = vec.fit_transform(df_train.clean_text)\n",
    "X_test = vec.transform(df_test.clean_text)\n",
    "\n",
    "y_train = df_train.target\n",
    "y_test = df_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95546155",
   "metadata": {},
   "source": [
    "### Train and evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893e66f",
   "metadata": {},
   "source": [
    "In the code above you used `CountVectorizer` to turn the text into numerical features. Here's what's happening:\n",
    "\n",
    "- **Lines 1 to 4:** You use `CountVectorizer` to build a [bag-of-words representation](https://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation) of the `clean_text` column so that a machine learning model can understand it. You specify two paramters: `ngram_range` and `stop_words`. `ngram_range` is the range of n-grams that the function will use. An n-gram is a sequence of n words. `(1, 3)` means that the function will use sequences of 1, 2, and 3 words. `stop_words` is a list of words that the function will ignore. In this case, the list \"english\" means that the function will ignore most common words in English.\n",
    "- **Lines 6 and 7:** You generate the matrices of token counts (bag-of-words) for your training and testing set and save them into `X_train` and `X_test`.\n",
    "- **Lines 9 and 10:** You save the response variable from the training and testing set into `y_train` and `y_test`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f2d4df",
   "metadata": {},
   "source": [
    "Finally, you just need to train the model by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b39f29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.94      0.53      0.68       160\n",
      "          misc.forsale       0.98      0.90      0.94       195\n",
      "             sci.space       0.93      0.92      0.92       197\n",
      "soc.religion.christian       0.65      0.97      0.78       200\n",
      "    talk.politics.guns       0.91      0.86      0.89       182\n",
      "\n",
      "              accuracy                           0.85       934\n",
      "             macro avg       0.88      0.84      0.84       934\n",
      "          weighted avg       0.88      0.85      0.85       934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "preds = nb.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308c834",
   "metadata": {},
   "source": [
    "In **lines 1 and 2** you train a [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) model. This is a simple probabilistic model that is commonly used when using discrete features such as word counts.\n",
    "\n",
    "Then, in **lines 4 and 5**, you evaluate the results of the model by computing the precision, recall, and f1 scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa18ad",
   "metadata": {},
   "source": [
    "### Saving and loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbed3cf",
   "metadata": {},
   "source": [
    "If you'd like to save the model for later, then use you can use `joblib`. Here's how you'd save the model you just finished training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41d19ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vec.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(nb, \"nb.joblib\")\n",
    "joblib.dump(vec, \"vec.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2221a1",
   "metadata": {},
   "source": [
    "Then, if you want to re-use your model later on, you can simply read it and use it to classify new samples of data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3375c2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sci.space'], dtype='<U22')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_saved = joblib.load(\"nb.joblib\")\n",
    "vec_saved = joblib.load(\"vec.joblib\")\n",
    "\n",
    "sample_text = [\"Space, Stars, Planets and Astronomy!\"]\n",
    "clean_sample_text = process_text(sample_text)\n",
    "sample_vec = vec_saved.transform(sample_text)\n",
    "nb_saved.predict(sample_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
